# -*- coding: utf-8 -*-
"""_gpu_fl-with-flower.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EX94UwZm3EM0pA5niGQmbWf5vZaTSiIs
"""

!pip install datasets evaluate flwr["simulation"] torch transformers

from collections import OrderedDict
import os
import random
import warnings
import sqlite3
import pandas as pd

import flwr as fl

import torch
from torch.utils.data import DataLoader

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

from datasets import load_dataset, Dataset, DatasetDict
from evaluate import load as load_metric

from transformers import LongformerForSequenceClassification, LongformerTokenizer, DataCollatorWithPadding, LongformerForSequenceClassification, AdamW, logging, get_scheduler

import pickle

from IPython.display import display

from google.colab import drive
drive.mount('/content/drive/')

warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
logging.set_verbosity(logging.ERROR)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.simplefilter('ignore')

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Treinamento será realizado em: {DEVICE}")
CHECKPOINT = "allenai/longformer-base-4096"  # transformer model checkpoint
NUM_CLIENTS = 2
NUM_ROUNDS = 2

tokenizer = LongformerTokenizer.from_pretrained(CHECKPOINT)

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding=True, max_length=4096)

# Conectando-se ao banco de dados SQLite3
conn = sqlite3.connect('/content/drive/MyDrive/Colab Notebooks/class_federa_processo/documentos_tratados_balanceado.db')

# Consultando a tabela para obter os dados
query = "SELECT cd_classe_judicial classe, texto FROM tabela_agrupada"
df = pd.read_sql(query, conn)

# Fechar a conexão com o banco de dados
conn.close()

# Codificar as classes
label_encoder = LabelEncoder()
df['classe_encoded'] = label_encoder.fit_transform(df['classe'])

# Salvar o LabelEncoder em um arquivo para uso posterior (se necessário)
with open('/content/drive/MyDrive/Colab Notebooks/class_federa_processo/label_encoder.pkl', 'wb') as f:
    pickle.dump(label_encoder, f)

# Dividir os dados em treino e validação
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['texto'].tolist(),
    df['classe_encoded'].tolist(),
    test_size=0.2,
    random_state=42
)

# Preparar os datasets
train_data = Dataset.from_dict({"text": train_texts, "label": train_labels})
val_data = Dataset.from_dict({"text": val_texts, "label": val_labels})

train_data = train_data.map(tokenize_function, batched=True)
val_data = val_data.map(tokenize_function, batched=True)

train_data = train_data.rename_column("label", "labels")
val_data = val_data.rename_column("label", "labels")

train_data.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
val_data.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
trainloader = DataLoader(
    train_data,
    shuffle=True,
    batch_size=4,
    collate_fn=data_collator,
)

testloader = DataLoader(
    val_data, batch_size=4, collate_fn=data_collator
)

def train(net, trainloader, epochs, learning_rate=5e-5, warmup_steps=500, gradient_accumulation_steps=2, weight_decay=0.01, device=DEVICE):
    # Inicializando o otimizador e o scheduler
    optimizer = AdamW(net.parameters(), lr=learning_rate, weight_decay=weight_decay)

    num_train_steps = len(trainloader) * epochs // gradient_accumulation_steps  # Total de passos de treinamento
    scheduler = get_scheduler(
        name="linear",  # Usar um scheduler linear
        optimizer=optimizer,
        num_warmup_steps=warmup_steps,  # Passos de aquecimento
        num_training_steps=num_train_steps  # Total de passos de treinamento
    )

    # Colocando o modelo no modo de treinamento
    net.train()

    global_step = 0  # Contador de passos globais (para controle de logging, etc.)

    for epoch in range(epochs):
        epoch_loss = 0  # Para registrar a perda da época

        # Passar por todos os lotes de treinamento
        for step, batch in enumerate(trainloader):
            batch = {k: v.to(device) for k, v in batch.items()}  # Mover os dados para o dispositivo

            # Fazer a previsão e calcular a perda
            outputs = net(**batch)
            loss = outputs.loss
            loss = loss / gradient_accumulation_steps  # Acumular o gradiente, se necessário

            loss.backward()  # Backpropagation

            # Acumulação de gradientes
            if (step + 1) % gradient_accumulation_steps == 0:
                optimizer.step()  # Atualizar os parâmetros
                scheduler.step()  # Atualizar o agendador de taxa de aprendizado
                optimizer.zero_grad()  # Limpar os gradientes

            epoch_loss += loss.item()  # Acumular a perda da época
            global_step += 1  # Incrementar o contador de passos globais

            # Log a cada 'logging_steps' passos
            if global_step % 10 == 0:
                print(f"Passo {global_step}, Perda: {loss.item():.4f}")

        # Média da perda da época
        avg_epoch_loss = epoch_loss / len(trainloader)
        print(f"Época {epoch+1} - Perda média: {avg_epoch_loss:.4f}")

        # Avaliar o modelo na validação após cada época (similar ao parâmetro 'evaluation_strategy="epoch"')
        net.eval()
        eval_loss = 0
        num_eval_steps = 0

        with torch.no_grad():
            for batch in trainloader:
                batch = {k: v.to(device) for k, v in batch.items()}
                outputs = net(**batch)
                eval_loss += outputs.loss.item()
                num_eval_steps += 1

        avg_eval_loss = eval_loss / num_eval_steps
        print(f"Época {epoch+1} - Perda de validação: {avg_eval_loss:.4f}")

        # Colocar o modelo de volta em modo de treinamento
        net.train()

    print("Treinamento Concluído!")

def test(net, testloader, output_dir="test_results.txt"):
    # Inicializar métrica de acurácia
    metric = load_metric("accuracy")
    all_preds = []
    all_labels = []

    # Colocar o modelo em modo de avaliação
    net.eval()

    # Inicializar o cálculo de perdas
    loss = 0

    # Criar ou limpar o arquivo de resultados
    with open(output_dir, "w") as f:
        # Passar pelos lotes de teste
        for batch in testloader:
            batch = {k: v.to(DEVICE) for k, v in batch.items()}

            # Desativa os gradientes para economizar memória
            with torch.no_grad():
                outputs = net(**batch)

            logits = outputs.logits
            loss += outputs.loss.item()

            # Predições
            predictions = torch.argmax(logits, dim=-1)

            # Armazenar as predições e os rótulos reais
            all_preds.extend(predictions.cpu().numpy())
            all_labels.extend(batch["labels"].cpu().numpy())

            # Atualizar as métricas de acurácia
            metric.add_batch(predictions=predictions, references=batch["labels"])

        # Calcular perda média
        loss /= len(testloader.dataset)

        # Calcular acurácia
        accuracy = metric.compute()["accuracy"]

        # Gerar o relatório de classificação
        report = classification_report(all_labels, all_preds, zero_division=0)

        # Gerar a matriz de confusão
        conf_matrix = confusion_matrix(all_labels, all_preds)

        # Salvar os resultados no arquivo
        f.write("Matriz de Confusão:\n")
        f.write(str(conf_matrix))
        f.write("\n\nRelatório de Classificação:\n")
        f.write(report)
        f.write(f"\nAcurácia: {accuracy:.2f}\n")
        f.write(f"Perda: {loss:.2f}\n")

    print(f"Resultados salvos em {output_dir}")

    return loss, accuracy, report

net = LongformerForSequenceClassification.from_pretrained(
    CHECKPOINT, num_labels=len(label_encoder.classes_)
).to(DEVICE)

class IMDBClient(fl.client.NumPyClient):
    def __init__(self, net, trainloader, testloader):
        self.net = net
        self.trainloader = trainloader
        self.testloader = testloader

    def get_parameters(self, config):
        return [val.cpu().numpy() for _, val in self.net.state_dict().items()]

    def set_parameters(self, parameters):
        params_dict = zip(self.net.state_dict().keys(), parameters)
        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})
        self.net.load_state_dict(state_dict, strict=True)

    def fit(self, parameters, config):
        self.set_parameters(parameters)
        print("Training Started...")
        train(self.net, self.trainloader, epochs=3)
        print("Training Finished.")
        return self.get_parameters(config={}), len(self.trainloader), {}

    def evaluate(self, parameters, config):
        # Carregar os parâmetros do modelo com base nos valores passados pelo Flower
        self.set_parameters(parameters)

        # Chamar a função 'test' que já calcula o classification_report, confusion_matrix, etc.
        loss, accuracy, report = test(self.net, self.testloader)

        # Garantir que os valores sejam do tipo esperado
        return float(loss), len(self.testloader), {
            "accuracy": float(accuracy),  # Acurácia como float
            "loss": float(loss),          # Perda como float
            "classification_report": str(report),  # Garantir que seja string
            #"confusion_matrix": conf_matrix.tolist()  # Já convertido para lista
        }

#trainloader, testloader = load_data()
def client_fn(cid):
  return IMDBClient(net, trainloader, testloader)

def weighted_average(metrics):
  accuracies = [num_examples * m["accuracy"] for num_examples, m in metrics]
  losses = [num_examples * m["loss"] for num_examples, m in metrics]
  examples = [num_examples for num_examples, _ in metrics]
  return {"accuracy": sum(accuracies) / sum(examples), "loss": sum(losses) / sum(examples)}

strategy = fl.server.strategy.FedAvg(
    fraction_fit=1.0,
    fraction_evaluate=1.0,
    evaluate_metrics_aggregation_fn=weighted_average,
)

fl.simulation.start_simulation(
    client_fn=client_fn,
    num_clients=NUM_CLIENTS,
    config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS),
    strategy=strategy,
    client_resources={"num_cpus": 1, "num_gpus": 1},
    ray_init_args={"log_to_driver": False, "num_cpus": 1, "num_gpus": 1}
)

# Supondo que `model` seja o seu modelo treinado
torch.save(net.state_dict(), '/content/drive/MyDrive/Colab Notebooks/class_federa_processo/model.pth')

# Salvar o modelo completo
net.save_pretrained('/content/drive/MyDrive/Colab Notebooks/class_federa_processo/longformer_model')
tokenizer.save_pretrained('/content/drive/MyDrive/Colab Notebooks/class_federa_processo/longformer_model')